[DEFAULT]
# Name of the experiment to run. All data would be stored
# in a directory with this name, and the best checkpoint
# Stored there also.
exp_name = MM-DistillNet
log_path = tensorboard
saved_path = "trained_models"
fast_run = False

# ================Input configuration====================
# Dataset
dataset=MultimodalDetection
#data_path=data
data_path=MAVD_dataset
id_filter=None
drive_type=all

# We are interested only in cars
valid_labels=car

# This decides if labels are taken from teacher or from the
# Dataset
use_labels=False

# Modalitis to be used on the teachers
use_thermal=True
use_depth=True
use_rgb=True
use_audio=False

# We show compeling results on audio, but the technique
# can be used for other modalities as well
student_modality=audio

# Sizes of the images. Selected EfficientDet expect 768
image_size = 768
thermal_size = 768
depth_size = 768
audio_size = 768

# Normalizes images to reduce exploding grads
normalize=True

# Transformations to use on each batched modality
train_transformations=Normalizer,Resizer
val_transformations=Normalizer,Resizer

# ================Train configuration====================
# Make run reproducible. Lesser than zero ignore this setting
seed = 24

batch_size = 2

# Enable Parallel Execution
ngpu = 2
num_workers=6
engine=DataParallel
#engine=DistributedDataParallel

# Teacher Network Type. Used for weights startup also
teacher=YetAnotherEfficientDet_D2
# The student uses an embedding to further improve audio-only
# performance
student=YetAnotherEfficientDet_D2_embedding
features_from=efficientnet

# Losses. Main loss provides classification and regression loss
# We do not find an ROI of using divergence loss nor adversarial
# Our loss is the MTALossLoss
main_loss=YetAnotherFocalLoss
div_loss=None
kd_loss=MTALoss
adv_loss=None
T=9
p=2

# Data augmentation can further improve performance
data_augment_shift=False

# Weights to penalize more certain losses
w_main=1.0
w_div=1.0
w_kd=0.005
w_adv=1.0

# Training Information
resume = True
# train_method=traditional_nms_augmented
train_method=traditional_nms_augmented_with_cross_attention
use_cross_attention=True
integration_mode='concat'
es_patience = 5
num_epoches = 50
val_interval = 5

# BOHB information
enable_bohb = False
bohb_iterations = 4
enable_prev_bohb_run=False

#Adversarial Training Tweaks
pretrain=False
weights_init=False

#Optimizer
grad_clip = -1
optimizer=Adam
lr = 1e-4
momentum=0.9
weight_decay=5e-4
b1=0.9
b2=0.999

# Scheduler
scheduler=ReduceLROnPlateau
step_size = 10
gamma=0.1

# Evaluation information
iou_thres = 0.5
conf_threshold = 0.3
nms_threshold = 0.5

#Cross-Attention
cross_attention_embed_dim = 768
cross_attention_num_heads = 8
cross_attention_dropout = 0.1